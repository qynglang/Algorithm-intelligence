{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11f6c7518>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pic_s=np.load('pic_s.npy')\n",
    "plt.imshow(pic_s[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_s[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A=1-convolve2d(pic_s[:,:,1]/255,np.ones((5,5)),'valid')/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1-convolve2d(pic_s[:,:,1]/255,np.ones((1,5)),'valid')/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1-convolve2d(pic_s[:,:,1]/255,np.ones((5,1)),'valid')/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D=1-convolve2d(pic_s[:,:,1]/255,np.eye(5),'valid')/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "EE=np.eye(5)\n",
    "E=1-convolve2d(pic_s[:,:,1]/255,EE[::-1],'valid')/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3072)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=np.hstack((A.reshape(1,576),B.reshape(1,672),C.reshape(1,672),D.reshape(1,576),E.reshape(1,576)))\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "#no softmax\n",
    "def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward propagates the network given an input batch\n",
    "    How it works is...\n",
    "\n",
    "\n",
    "    :param x: Inputs x (b, c, h, w)\n",
    "    :return: preds (b, num_classes)\n",
    "    \"\"\"\n",
    "    minibatch_size = x.shape[0]\n",
    "    num_panels = x.shape[1]\n",
    "    cnn_input_shape = (minibatch_size * num_panels, 1, self.input_shape[-2], self.input_shape[-1])\n",
    "    out = x.view(cnn_input_shape) # reshape into (minibatch_size * 9, 1, h, w) as we don't convolute across panels\n",
    "\n",
    "    # Beginning of CNN\n",
    "    for i in range(self.num_layers):  # for number of CNN layers\n",
    "        print(i,out.shape)\n",
    "        out = self.layer_dict['conv_{}'.format(i)](out)  # pass through conv layer indexed at i\n",
    "        out = F.relu(out)  # pass conv outputs through ReLU\n",
    "        #out = self.batchnorm(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward propages the network given an input batch\n",
    "    :param x: Inputs x (b, c, h, w)\n",
    "    :return: preds (b, num_classes)\n",
    "    \"\"\"\n",
    "    out = x\n",
    "    for i in range(self.num_layers):  # for number of layers\n",
    "\n",
    "        out = self.layer_dict['conv_{}'.format(i)](out)  # pass through conv layer indexed at i\n",
    "        out = F.relu(out)  # pass conv outputs through ReLU\n",
    "        if self.dim_reduction_type == 'strided_convolution':  # if strided convolution dim reduction then\n",
    "            out = self.layer_dict['dim_reduction_strided_conv_{}'.format(i)](\n",
    "                out)  # pass previous outputs through a strided convolution indexed i\n",
    "            out = F.relu(out)  # pass strided conv outputs through ReLU\n",
    "\n",
    "        elif self.dim_reduction_type == 'dilated_convolution':\n",
    "            out = self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)](out)\n",
    "            out = F.relu(out)\n",
    "\n",
    "        elif self.dim_reduction_type == 'max_pooling':\n",
    "            out = self.layer_dict['dim_reduction_max_pool_{}'.format(i)](out)\n",
    "\n",
    "        elif self.dim_reduction_type == 'avg_pooling':\n",
    "            out = self.layer_dict['dim_reduction_avg_pool_{}'.format(i)](out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building basic block of ConvolutionalNetwork using input shape (1, 1, 28, 28)\n",
      "torch.Size([1, 5, 28, 28])\n",
      "torch.Size([1, 5, 15, 15])\n",
      "torch.Size([1, 5, 15, 15])\n",
      "torch.Size([1, 5, 8, 8])\n",
      "torch.Size([1, 5, 8, 8])\n",
      "torch.Size([1, 5, 5, 5])\n",
      "torch.Size([1, 5, 5, 5])\n",
      "torch.Size([1, 5, 3, 3])\n",
      "shape before final linear layer torch.Size([1, 5, 2, 2])\n",
      "Block is built, output volume is torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "CN=ConvolutionalNetwork((1,1,28,28), 'max_pooling', 1, 5, 4, use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3, 3])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.Tensor(pic_s[:,:,1].reshape(1,1,28,28)).float().to(device='cpu')\n",
    "out=forward(CN, x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, dim_reduction_type, num_output_classes, num_filters, num_layers, use_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes a convolutional network module object.\n",
    "        :param input_shape: The shape of the inputs going in to the network.\n",
    "        :param dim_reduction_type: The type of dimensionality reduction to apply after each convolutional stage, should be one of ['max_pooling', 'avg_pooling', 'strided_convolution', 'dilated_convolution']\n",
    "        :param num_output_classes: The number of outputs the network should have (for classification those would be the number of classes)\n",
    "        :param num_filters: Number of filters used in every conv layer, except dim reduction stages, where those are automatically infered.\n",
    "        :param num_layers: Number of conv layers (excluding dim reduction stages)\n",
    "        :param use_bias: Whether our convolutions will use a bias.\n",
    "        \"\"\"\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_reduction_type = dim_reduction_type\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        print(\"Building basic block of ConvolutionalNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))  # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        for i in range(self.num_layers):  # for number of layers times\n",
    "            self.layer_dict['conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                             # add a conv layer in the module dict\n",
    "                                                             kernel_size=3,\n",
    "                                                             out_channels=self.num_filters, padding=1,\n",
    "                                                             bias=self.use_bias)\n",
    "\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out)  # use layer on inputs to get an output\n",
    "            out = F.relu(out)  # apply relu\n",
    "            print(out.shape)\n",
    "            if self.dim_reduction_type == 'strided_convolution':  # if dim reduction is strided conv, then add a strided conv\n",
    "                self.layer_dict['dim_reduction_strided_conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                                                       kernel_size=3,\n",
    "                                                                                       out_channels=out.shape[1],\n",
    "                                                                                       padding=1,\n",
    "                                                                                       bias=self.use_bias, stride=2,\n",
    "                                                                                       dilation=1)\n",
    "\n",
    "                out = self.layer_dict['dim_reduction_strided_conv_{}'.format(i)](\n",
    "                    out)  # use strided conv to get an output\n",
    "                out = F.relu(out)  # apply relu to the output\n",
    "            elif self.dim_reduction_type == 'dilated_convolution':  # if dim reduction is dilated conv, then add a dilated conv, using an arbitrary dilation rate of i + 2 (so it gets smaller as we go, you can choose other dilation rates should you wish to do it.)\n",
    "                self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                                                       kernel_size=3,\n",
    "                                                                                       out_channels=out.shape[1],\n",
    "                                                                                       padding=1,\n",
    "                                                                                       bias=self.use_bias, stride=1,\n",
    "                                                                                       dilation=i + 2)\n",
    "                out = self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)](\n",
    "                    out)  # run dilated conv on input to get output\n",
    "                out = F.relu(out)  # apply relu on output\n",
    "\n",
    "            elif self.dim_reduction_type == 'max_pooling':\n",
    "                self.layer_dict['dim_reduction_max_pool_{}'.format(i)] = nn.MaxPool2d(2, padding=1)\n",
    "                out = self.layer_dict['dim_reduction_max_pool_{}'.format(i)](out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'avg_pooling':\n",
    "                self.layer_dict['dim_reduction_avg_pool_{}'.format(i)] = nn.AvgPool2d(2, padding=1)\n",
    "                out = self.layer_dict['dim_reduction_avg_pool_{}'.format(i)](out)\n",
    "\n",
    "            print(out.shape)\n",
    "        if out.shape[-1] != 2:\n",
    "            out = F.adaptive_avg_pool2d(out, 2)  # apply adaptive pooling to make sure output of conv layers is always (2, 2) spacially (helps with comparisons).\n",
    "        print('shape before final linear layer', out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        self.logit_linear_layer = nn.Linear(in_features=out.shape[1],  # add a linear layer\n",
    "                                            out_features=self.num_output_classes,\n",
    "                                            bias=self.use_bias)\n",
    "        out = self.logit_linear_layer(out)  # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propages the network given an input batch\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for i in range(self.num_layers):  # for number of layers\n",
    "\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out)  # pass through conv layer indexed at i\n",
    "            out = F.relu(out)  # pass conv outputs through ReLU\n",
    "            if self.dim_reduction_type == 'strided_convolution':  # if strided convolution dim reduction then\n",
    "                out = self.layer_dict['dim_reduction_strided_conv_{}'.format(i)](\n",
    "                    out)  # pass previous outputs through a strided convolution indexed i\n",
    "                out = F.relu(out)  # pass strided conv outputs through ReLU\n",
    "\n",
    "            elif self.dim_reduction_type == 'dilated_convolution':\n",
    "                out = self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)](out)\n",
    "                out = F.relu(out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'max_pooling':\n",
    "                out = self.layer_dict['dim_reduction_max_pool_{}'.format(i)](out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'avg_pooling':\n",
    "                out = self.layer_dict['dim_reduction_avg_pool_{}'.format(i)](out)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBC(nn.Module):\n",
    "    def __init__(self, input_shape, dim_reduction_type = None, num_filters = 32, num_layers = 4, use_bias=True, rnn_hidden_units = 64, num_rnn_layers = 1, learning_rate = 1e-4, stride = 2, padding = 0, num_questions = 4, num_panels=9):\n",
    "        \"\"\"\n",
    "        Learning By Contrasting for Visual Analogy problems\n",
    "        Initialises the network for visual analogy problems as in [1]\n",
    "        :param input_shape: Default is 80 NOTE we may be fed 160 x 160 images\n",
    "\n",
    "        Details\n",
    "        -------\n",
    "        This initialises the CNN -> RNN model that is used for the visual analogy problems.\n",
    "        The architecture is outlined in section 7.1 in [1].\n",
    "        \n",
    "        Input:\n",
    "        80 x 80 greyscale images (arrays of ints).\n",
    "        Because we have an RNN at the other end, training input is given as four sequences. Moreover, we expect input to be really a 9 x 80 x 80 tensor, and we are supposed to construct 4 tensors out of it.\n",
    "        CNN:\n",
    "        4 layers deep, 32 kernels per layer, each 3 x 3 with stride 2.\n",
    "        RNN:\n",
    "        64 hidden units.\n",
    "        Optimiser:\n",
    "        Adam with lr = 1e-4\n",
    "        -------------------\n",
    "        References:\n",
    "        [1] Hill et al, Learning to Make Analogies by Contrasting Abstract Relational Structure\n",
    "        \n",
    "        \"\"\"\n",
    "        super(LBC, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape # this should be (minibatch_size, 9, 80, 80)\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = 1 # the output should just be a scalar\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.rnn_hidden_units = rnn_hidden_units\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.num_panels = num_panels # how many panels in the raw input (should be 9)\n",
    "        self.num_questions = num_questions # how many questions are asked, should be 4 (1 per questio panel)\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        print(\"Building basic block of LBC using input shape\", self.input_shape)\n",
    "        minibatch_size = self.input_shape[0]        \n",
    "        cnn_input_shape = (minibatch_size * self.num_panels, 1, self.input_shape[-2], self.input_shape[-1]) # since we give panels independently to CNN, we /do not/ combine channels (i.e. panels) through the convolutions\n",
    "        print(\"Actual shape accepted for CNN:\", cnn_input_shape)\n",
    "\n",
    "        # First build the CNN at the head\n",
    "        x = torch.zeros((cnn_input_shape))  # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm2d(self.num_filters) # should be run after every convlayer\n",
    "        for i in range(self.num_layers):  # for number of layers times\n",
    "            self.layer_dict['conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                             # add a conv layer in the module dict\n",
    "                                                             kernel_size=3,\n",
    "                                                             out_channels=self.num_filters,\n",
    "                                                             padding=self.padding,\n",
    "                                                             bias=self.use_bias,\n",
    "                                                             stride = self.stride)\n",
    "\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out)  # use layer on inputs to get an output\n",
    "            out = F.relu(out)  # apply relu\n",
    "            # out = self.batchnorm(out) # doesn't affect dimensions\n",
    "            print(out.shape)\n",
    "        # if out.shape[-1] != 2:\n",
    "        #     out = F.adaptive_avg_pool2d(out,\n",
    "        #                                 2)  # apply adaptive pooling to make sure output of conv layers is always (2, 2) spacially (helps with comparisons).\n",
    "        print('shape after CNN', out.shape) # should be 4 x 4 in the last two dims\n",
    "\n",
    "        # Now we build the RNN behind the CNN\n",
    "        # We expect there to be four different inputs, each input having 6 embeddings, so for the RNN an individual minibatch is really a question\n",
    "        # But since hidden states are not kept across questions, we might as well do everything in batches, so in fact RNN expects input as num_questions * minibatch_size\n",
    "        # Note that \"out\" will be flattened to be only 3 dimensional for the LSTM\n",
    "        out = torch.zeros((self.num_questions * minibatch_size, 6, out.shape[-3]*out.shape[-2]*out.shape[-1]))\n",
    "        print(\"shape before RNN\", out.shape)\n",
    "        self.rnn = nn.LSTM(input_size = out.shape[-1],\n",
    "                           hidden_size = self.rnn_hidden_units,\n",
    "                           num_layers = self.num_rnn_layers,\n",
    "                           dropout=0.2,\n",
    "                           batch_first = True)\n",
    "        out = self.rnn(out)[1][0] # this is the last hidden state\n",
    "        out = out.view(self.input_shape[0], self.num_questions, -1)\n",
    "        print(\"shape after RNN\", out.shape)\n",
    "        self.logit_linear_layer = nn.Linear(in_features=out.shape[-1],  # add a linear layer\n",
    "                                            out_features=self.num_output_classes,\n",
    "                                            bias=self.use_bias)\n",
    "        out = self.logit_linear_layer(out).view(self.input_shape[0], self.num_questions)  # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagates the network given an input batch\n",
    "        How it works is...\n",
    "        \n",
    "\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        minibatch_size = x.shape[0]\n",
    "        num_panels = x.shape[1]\n",
    "        cnn_input_shape = (minibatch_size * num_panels, 1, self.input_shape[-2], self.input_shape[-1])\n",
    "        out = x.view(cnn_input_shape) # reshape into (minibatch_size * 9, 1, h, w) as we don't convolute across panels\n",
    "        \n",
    "        # Beginning of CNN\n",
    "        for i in range(self.num_layers):  # for number of CNN layers\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out)  # pass through conv layer indexed at i\n",
    "            out = F.relu(out)  # pass conv outputs through ReLU\n",
    "            out = self.batchnorm(out) # this seems to be helpful\n",
    "        # End of CNN\n",
    "        ####################\n",
    "        # Now we generate the questions for RNN input\n",
    "        # Reshape data so that we get batch numbering back, and flatten for RNN\n",
    "        out = out.view(minibatch_size, 1, num_panels, -1) # the second index indicates number of queries\n",
    "        # To process queries into questions: format is that everyone gets the first five panels (sample, then query), then exactly 1 from the remaning 4\n",
    "        # To do this we cut off the choices from the sample + query part. Then, we concatenate a transposed version of the choices to the sample+ query part\n",
    "        sample_and_query = out[:, :, 0:5].repeat([1, 4, 1, 1]) # sample and queries, repeated\n",
    "        choices_transposed = out[:, :, 5:].transpose(1, 2)\n",
    "        out = torch.cat((sample_and_query, choices_transposed),\n",
    "                                dim=2) # now out should be shape (x, 4, 6, x)\n",
    "        out = out.view(minibatch_size * 4, 6, -1) # since the RNN works independently; the above could have been done a bit quicker to elminate this operation, but this operation is cheap and the above is clearer as a result\n",
    "#         ####################\n",
    "#         # We now run the RNN, then the linear layer, then the softmax\n",
    "        out = self.rnn(out)\n",
    "        #out = self.rnn(out)[1][0]\n",
    "        #out = out.view(minibatch_size, self.num_questions, -1)\n",
    "        #out = self.logit_linear_layer(out).view(minibatch_size, self.num_questions)\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the network parameters.\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            try:\n",
    "                item.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        self.rnn.reset_parameters()\n",
    "        self.logit_linear_layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255., 255., 255., 255., 255., 255., 255., 255., 253., 249., 254.,\n",
       "        255., 255., 255., 255., 255., 255., 255., 255., 255., 227., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 251., 236., 223., 247.,\n",
       "        255., 255., 255., 255., 255., 255., 255., 226., 125., 187., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 253., 233., 178., 183., 237.,\n",
       "        255., 255., 255., 255., 255., 255., 255., 168.,  92., 199., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 253., 217., 150., 184., 241.,\n",
       "        255., 255., 255., 255., 255., 255., 229., 102., 175., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 249., 203., 152., 215., 252.,\n",
       "        255., 255., 255., 255., 255., 255., 186.,  89., 251., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 244., 190., 157., 236., 254.,\n",
       "        255., 255., 255., 255., 255., 255., 132., 142., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 246., 181., 159., 233., 255.,\n",
       "        255., 255., 255., 255., 255., 255., 115., 181., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 250., 189., 161., 226., 254.,\n",
       "        255., 255., 255., 255., 255., 255., 132., 170., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 254., 210., 154., 206., 251.,\n",
       "        255., 255., 255., 255., 255., 255., 116., 119., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 228., 147., 170., 244.,\n",
       "        255., 255., 254., 255., 255., 255., 171., 105., 255., 255., 255.,\n",
       "        253., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 243., 181., 147., 212.,\n",
       "        245., 252., 253., 255., 255., 255., 211.,  81., 162., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 252., 220., 162., 154.,\n",
       "        199., 219., 233., 248., 255., 255., 255., 173., 123., 188., 216.,\n",
       "        255., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 248., 218., 167.,\n",
       "        136., 137., 164., 215., 251., 255., 255., 232., 107.,  58.,  94.,\n",
       "        192., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 247., 214.,\n",
       "        140.,  79., 101., 170., 233., 252., 255., 255., 169.,  38.,  66.,\n",
       "        147., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 244.,\n",
       "        201., 137., 110., 128., 184., 236., 254., 255., 230., 121.,  87.,\n",
       "        107., 215., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 253.,\n",
       "        242., 222., 194., 162., 146., 188., 235., 254., 255., 255., 176.,\n",
       "        109., 129., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        254., 254., 247., 231., 187., 149., 201., 249., 255., 255., 255.,\n",
       "        217.,  90., 187., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 251., 227., 171., 167., 230., 252., 255., 255.,\n",
       "        242., 136., 149., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 248., 210., 156., 209., 248., 255., 255.,\n",
       "        255., 192., 112., 232., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 253., 232., 169., 192., 243., 255., 255.,\n",
       "        255., 224.,  98., 228., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 254., 236., 177., 181., 241., 254., 255.,\n",
       "        255., 224.,  98., 228., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 237., 174., 181., 245., 255., 255.,\n",
       "        255., 198., 112., 232., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 254., 238., 170., 188., 244., 255., 255.,\n",
       "        255., 155., 149., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 253., 223., 162., 210., 251., 255., 255.,\n",
       "        242., 119., 187., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 244., 191., 158., 222., 254., 255., 253.,\n",
       "        142., 130., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 254., 230., 174., 183., 239., 254., 253., 165.,\n",
       "        117., 198., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 253., 238., 215., 225., 251., 255., 242., 191.,\n",
       "        172., 255., 255., 255., 255., 255.],\n",
       "       [255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 253., 251., 253., 255., 255., 255., 255.,\n",
       "        255., 255., 255., 255., 255., 255.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def activation(arr,kernel):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
